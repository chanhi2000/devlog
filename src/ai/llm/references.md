---
lang: ko-KR
title: References
description: LLM > References
icon: fas fa-book-atlas
category: 
  - LLM
  - References
tag: 
  - ai
  - llm
  - llama
head:
  - - meta:
    - property: og:title
      content: LLM > References
    - property: og:description
      content: References
    - property: og:url
      content: https://chanhi2000.github.io/ai/llm/references.html
---

# {{ $frontmatter.title }} 관련

[[toc]]

---

## <VPIcon icon="fa-brands fa-google"/>Google

### Colab

- [Knowledge Distillation](https://colab.research.google.com/drive/1iw3TPRBSXOd5EgXLJbTiN6uFO1mCoq6C)
- [`Selecting_an_embedding_model_for_custom_data.ipynb`](https://colab.research.google.com/drive/132oXSGSOyzZ7GO9pJhRKlvwY4F-i9Pm6)

<!-- END: colab.research.google.com -->

---

## pdf(s)

- [OLMo : Accelerating the Science of Language Models](https://allenai.org/olmo/olmo-paper.pdf)

<PDF url="https://arxiv.org/pdf/2306.11025" />
<PDF url="https://arxiv.org/pdf/2404.07143" />
<PDF url="https://arxiv.org/pdf/2404.14219" />
<PDF url="https://arxiv.org/pdf/2407.12994" />
<PDF url="https://arxiv.org/pdf/2407.21059" />
<PDF url="https://arxiv.org/pdf/2408.11039" />
<PDF url="https://arxiv.org/pdf/2409.12089" />
<PDF url="https://arxiv.org/pdf/1503.02531" />
<PDF url="https://arxiv.org/pdf/2407.16833" />
<PDF url="https://arxiv.org/pdf/2410.02525" />
<PDF url="https://arxiv.org/pdf/2409.14924" />
<PDF url="https://arxiv.org/pdf/2402.14207" />
<PDF url="https://arxiv.org/pdf/2409.16694" />
<PDF url="https://arxiv.org/pdf/2409.16416" />
<PDF url="https://arxiv.org/pdf/2409.15173" />
<PDF url="https://arxiv.org/pdf/2409.14924" />
<PDF url="https://arxiv.org/pdf/2402.10200" />

---

## <VPIcon icon="fa-brands fa-x-twitter"/>X

- [`ariG23498` / A simple hack to calculating how much VRAM you would need to run a model.](https://x.com/ariG23498/status/1840967205699367116)
- [`mervenoyann` / NVIDIA just dropped a gigantic multimodal model called NVLM 72B 🦖](https://x.com/mervenoyann/status/1841098941900767323)
- [`danielhanchen` / Llama 3.2 Multimodal benchmarks](https://x.com/danielhanchen/status/1838991771948425652)
- [`ArtificialAnlys` / OpenAI’s o1-preview is the first model to substantially push the frontier of language model intelligence since the original GPT-4 over 18 months ago](https://x.com/ArtificialAnlys/status/1838955372855390618)
- [`helloiamleonie` / Chunking techniques for RAG:](https://x.com/helloiamleonie/status/1838760385224089769)
- [`akshay_pachaar` / Multimodal Retrieval Augmented Generation (RAG), clearly explained:](https://x.com/akshay_pachaar/status/1838556536051999135)
- [`weaviate_io` / 6 types of vector embeddings for your AI applications](https://x.com/weaviate_io/status/1842203043724431775)

<!-- END: x.com -->

---

## NVIDIA

- [NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models](https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/)

---

## <VPIcon icon="fa-brands fa-medium"/>Medium

- [`llamaindex` /  Introducing LlamaCloud and LlamaParse](https://blog.llamaindex.ai/introducing-llamacloud-and-llamaparse-af8cedf9006b)
- [`berom0227` / Running Ollama in Smart Connections](https://berom0227.medium.com/running-ollama-in-smart-connections-db2242aee3ba)
- [`thedeephub` / 50+ Open-Source Options for Running LLMs Locally](https://medium.com/thedeephub/50-open-source-options-for-running-llms-locally-db1ec6f5a54f)
- [`aksh-garg` / Llama 3-V: Matching GPT4-V with a 100x smaller model and 500 dollars](https://aksh-garg.medium.com/llama-3v-building-an-open-source-gpt-4v-competitor-in-under-500-7dd8f1f6c9ee)
- [`tpbabparn` / In-house LLM-application by Spring AI + Ollama](https://tpbabparn.medium.com/in-house-llm-application-by-spring-ai-ollama-91c48e2d2d38)
- [`jhk0530` / LG의 오픈소스 AI, 엑사원 3.0 사용후기](https://jhk0530.medium.com/lg-exaone-3-0-0e7221db6356?source=rss-cb820693bed5------2)
- [`datastrato` / Building A Universal Data Agent in 15 Minutes with LlamaIndex and Apache Gravitino (incubating)](https://medium.com/datastrato/building-a-universal-data-agent-in-15-minutes-with-llamaindex-and-apache-gravitino-incubating-401ea24a3b39)
- [`sarmadafzalj` / Visualize Vector Embeddings in a RAG System](https://medium.com/@sarmadafzalj/visualize-vector-embeddings-in-a-rag-system-89d0c44a3be4)

<!-- END: medium.com -->

---

## <VPIcon icon="iconfont icon-velog"/>velog

- [`@geoffyoon-dev` - 데이터는 못 보내지만 Cloud LLM은 쓰고싶어](https://velog.io/@geoffyoon-dev/cloud-LLM-in-data-security-policy)
- [`@devstone` - NLP 메트릭 톺아보기](https://velog.io/@devstone/NLP-%EB%A9%94%ED%8A%B8%EB%A6%AD-%ED%86%BA%EC%95%84%EB%B3%B4%EA%B8%B0)
- [`@kwon0koang` / 오픈소스 LLM으로 RAG 에이전트 만들기 (랭체인, Ollama, Tool Calling 대체)](https://velog.io/@kwon0koang/%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4-LLM%EC%9C%BC%EB%A1%9C-RAG-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0)
- [`@eunbibi` / 자연어처리](https://velog.io/@eunbibi/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC)
- [`@euisuk-chung` / \[NLP\] 1. Introduction to Text Analytics](https://velog.io/@euisuk-chung/NLP-Text-Analytics-Intro)
- [`@euisuk-chung` / \[NLP\] 2. Steps of Text Analytics](https://velog.io/@euisuk-chung/%ED%85%8D%EC%8A%A4%ED%8A%B8-%EB%B6%84%EC%84%9D-%EB%8B%A8%EA%B3%84)
- [`@euisuk-chung` / \[꿀팁\] 프롬프트 엔지니어링 (강의 요약)](https://velog.io/@euisuk-chung/%EA%BF%80%ED%8C%81-%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-%EA%B0%95%EC%9D%98-%EC%9A%94%EC%95%BD)
- [`@euisuk-chung` / \[IT\] LLMOps와 RAG](https://velog.io/@euisuk-chung/IT-LLMOps%EC%99%80-RAG)
- [`@euisuk-chung` / 2025년 AI 트렌드: LMM, LAM, 온디바이스 AI, AI 에이전트, 임베디드 AI, 그리고 FMOps](https://velog.io/@euisuk-chung/2025%EB%85%84-AI-%ED%8A%B8%EB%A0%8C%EB%93%9C-LMM-LAM-%EC%98%A8%EB%94%94%EB%B0%94%EC%9D%B4%EC%8A%A4-AI-AI-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9E%84%EB%B2%A0%EB%94%94%EB%93%9C-AI-%EA%B7%B8%EB%A6%AC%EA%B3%A0-FMOps)
- [`@euisuk-chung` / \[트렌드\] 트렌스포머 이후의 차세대 아키텍쳐: MoE, SSM, RetNet, V-JEPA](https://velog.io/@euisuk-chung/%ED%8A%B8%EB%A0%8C%EB%93%9C-%ED%8A%B8%EB%A0%8C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EC%9D%B4%ED%9B%84%EC%9D%98-%EC%B0%A8%EC%84%B8%EB%8C%80-%EC%95%84%ED%82%A4%ED%85%8D%EC%B3%90-MoE-SSM-RetNet-V-JEPA)
- [`@euisuk-chung` / \[NLP\] 5. 자연어 차원 축소(Dimension Reduction) 기법](https://velog.io/@euisuk-chung/NLP-Dimension-Reduction-Methods)

<!-- END: velog.io -->

---

## Substack

- [`aiencoder` / GraphRAG Analysis, Part 1: How Indexing Elevates Knowledge Graph Performance in RAG](https://aiencoder.substack.com/p/graphrag-analysis-part-1-how-indexing)
- [`weightythoughts` / Consider the Llama](https://weightythoughts.com/p/consider-the-llama)

---

## Brunch

- [`@ZVA` / 어텐션맵은 뭐고 트랜스포머는 또 뭐냐...LLM 입문](https://brunch.co.kr/@@ZVA/702)
- [`@2YWz` / Enterprise LLM 사용자 인터페이스](https://brunch.co.kr/@@2YWz/114)
- [`@5jl5` / \[책소개\] 금융 AI의 이해  - 신용 평가, 사기 탐지, 퀀트투자, 생성형 AI를 활용한 실전 금융 AI](https://brunch.co.kr/@@5jl5/127)
- [`@ZVA` / 자바의 아버지 제임스 고슬링, 생성AI를 생각한다](https://brunch.co.kr/@@ZVA/722)
- [`@ZVA` / 생성AI와 읽기의 종말 시대, 가르친다는 것에 대하여](https://brunch.co.kr/@@ZVA/737)
- [`@5jl5` / RAG기반 LLM서비스 전망](https://brunch.co.kr/@@5jl5/132)
- [`@5jl5` / \[책소개\] Transformers & LLMs 그림책 - Super Study Guide: Transformers & LLMs](https://brunch.co.kr/@@5jl5/133)
- [`@ZVA` / 벡터 임베딩 기반 RAG는 왜 실패하는가](https://brunch.co.kr/@@ZVA/752)
- [`@2rV` / 누구나 참여할 수 있는 프로토타이핑 - AI 코드 제너레이터와 함께하는 새로운 개발 문화](https://brunch.co.kr/@@2rV/198)

<!-- END: brunch.co.kr -->

---

## inblog

- [`graphwoody` / From RAG to GraphRAG , What is the GraphRAG and why i use it?](https://inblog.ai/graphwoody/GraphRAG-01)

---

## Replicate

- [A comprehensive guide to running Llama 2 locally](https://replicate.com/blog/run-llama-locally)

---

## MetaAI

- [Open sourcing AudioCraft: Generative AI for audio made simple and available to all](https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio)
- [Introducing Code Llama, a state-of-the-art large language model for coding](https://ai.meta.com/blog/code-llama-large-language-model-coding)
- [Self-supervised learning: The dark matter of intelligence](https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence)
- [Audiobox: Generating audio from voice and natural language prompts](https://ai.meta.com/blog/audiobox-generating-audio-voice-natural-language-prompts)

---

## Google

### Colab

- [MANATEE(lm) : Market Analysis based on language model architectures](https://colab.research.google.com/drive/1Nq28vk9_l0R-53T18HYfRbeGFJoZ_U8E)
- [<VPIcon icon="fa-brands fa-python"/>`llm-pricing-cost-quality.ipynb`](https://colab.research.google.com/gist/virattt/7b67c685ca6b256d4fa6108bfae53d7a/exploring-llm-pricing-cost.ipynb)
- [<VPIcon icon="fa-brands fa-python"/>`cudf_pandas_stocks_demo.ipynb`](https://colab.research.google.com/github/rapidsai-community/showcase/blob/main/getting_started_tutorials/cudf_pandas_stocks_demo.ipynb)

---

## Ollama

- [Run Llama 2 Uncensored Locally](https://ollama.ai/blog/run-llama2-uncensored-locally)
- [Llama 3 is not very censored](https://ollama.com/blog/llama-3-is-not-very-censored)

---

## Linkedin

- [How to Fit Large Language Models in Small Memory: Quantization](https://www.linkedin.com/pulse/how-fit-large-language-models-small-memory-ivan-reznikov/)

---

## Finbarr Timbers

- [How is LLaMa.cpp possible?](https://finbarr.ca/how-is-llama-cpp-possible)

---

## fast.ai

- [Can LLMs learn from a single example?](https://www.fast.ai/posts/2023-09-04-learning-jumps)

---

## replit

- [I'm not a programmer, and I used AI to build my first bot](https://blog.replit.com/building-my-first-slack-bot)

---

## Second State

- [Fast and Portable Llama2 Inference on the Heterogeneous Edge](https://www.secondstate.io/articles/fast-llm-inference/)

---

## Bbycroft

- [LLM Visualization](https://bbycroft.net/llm)

---

## Justine

- [Bash One-Liners for LLMs](https://justine.lol/oneliners)
  - [<VPIcon icon="iconfont icon-shell"/>`jart/rename-pictures.sh`](https://gist.github.com/jart/bd2f603aefe6ac8004e6b709223881c0)

---

## moomou

- [Listening with LLM](https://paul.mou.dev/posts/2023-12-31-listening-with-llm/)

---

## Trail of Bits

- [LeftoverLocals: Listening to LLM responses through leaked GPU local memory](https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory)

---

## 2MB codes

- [`cmb/ollama-bot`: This is a rudimentary IRC bot that communicates with a local instance of ollama.](https://2mb.codes/~cmb/ollama-bot/)

---

## Shyam's Blog

- [Beyond Self-Attention: How a Small Language Model Predicts the Next Token](https://shyam.blog/posts/beyond-self-attention/)

---

## Gradient Descent into Madness

- [LLM from scratch: Automatic Differentiation](https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/post.html)

---

## Hamel

- [Fuck You, Show Me The Prompt.](https://hamel.dev/blog/posts/prompt/)

---

## Ahead of AI

- [Optimizing LLMs From a Dataset Perspective](https://sebastianraschka.com/blog/2023/optimizing-LLMs-dataset-perspective.html)
- [Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch](https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch)

---

## kapa.ai

- [Optimizing Technical Docs for LLMs](https://docs.kapa.ai/blog/optimizing-technical-documentation-for-llms)

---

## Cloudflare

- [Cloudflare announces Firewall for AI](https://blog.cloudflare.com/firewall-for-ai)

---

## Martin Lumiste

- [Compressing images with neural networks](https://mlumiste.com/technical/compression-deep-learning/)

---

## Brandon's Digital Garden

- [Building an email to calendar LLM](https://ngacho.com/blog/emaillm/)

---

## Justine Tunney's Web Page

- [LLaMA Now Goes Faster on CPUs](https://justine.lol/matmul/)

---

## hiddenest

- [llm은 모르지만 뭔가 만들고 싶어서](https://hiddenest.dev/three-hours-of-llm)

---

## tistory

- [`iostream` / Make headway towards solving the problem](https://iostream.tistory.com/m/)
  - [Full Stack Optimization of Transformer Inference: a Survey (1)](https://iostream.tistory.com/m/179)
  <!-- END: iostream -->
- [`cori` / 코딩하는 오리](https://cori.tistory.com/m/)
  - [나의 개발 일지 (3) RAG 구현 및 개선](https://cori.tistory.com/m/347)
  <!-- END: cori -->
- [`soohey` / 개발 아카이빙](https://soohey.tistory.com/m/)
  - [벤치마크 데이터셋에 대해](https://soohey.tistory.com/m/86)
  - [실전 LLM chp1](https://soohey.tistory.com/m/87)
  <!-- END: soohey -->
- [`csj000714` / 드프 DrawingProcess](https://csj000714.tistory.com/m/)
  - [\[Gen AI\] 생성형 모델들의 원리 비교: VAE, GAN, Flow-based, Diffusion](https://csj000714.tistory.com/m/1199)
  - [\[Gen AI\] 생성형 모델 및 서비스 정리](https://csj000714.tistory.com/m/1150)
  - [\[논문리뷰\] LiDAR2Map: LiDAR-based distillation scheme - LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation (CVPR 2023)](https://csj000714.tistory.com/m/1156)
  - [\[Gen AI\] Generative Adversarial Network(GAN) 설명: 기초](https://csj000714.tistory.com/m/1201)
  - [\[논문 리뷰\] HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting (CVPR 2024)](https://csj000714.tistory.com/m/1232)
  - [\[논문 리뷰\] HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting (CVPR 2024)](https://csj000714.tistory.com/m/1236)
  - [\[논문리뷰\] Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis (CVPR 2024)](https://csj000714.tistory.com/m/1240)
  - [\[논문리뷰\] DINO: Emerging Properties in Self-Supervised Vision Transformers (ICCV 2021)](https://csj000714.tistory.com/m/1244)
  - [\[논문 리뷰\] D-NeRF: Neural Radiance Fields for Dynamic Scenes (CVPR 2021)](https://csj000714.tistory.com/m/1248)
  - [\[Career\] CES 2025 사전 준비 및 관람](https://csj000714.tistory.com/m/1229)
  - [\[Project\] 2024 경희대학교 SW페스티벌: 피지컬 컴퓨팅 분야(24.11.27.)](https://csj000714.tistory.com/m/1251)
  - [\[논문 리뷰\] HyperNeRF : A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields (ACM TG 2021)](https://csj000714.tistory.com/m/1253)
  - [\[논문 리뷰\] 4D Gaussian Splatting: 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering (CVPR 2024)](https://csj000714.tistory.com/m/1254)
  - [\[논문 리뷰\] Ha-NeRF: NeRFwithRealWorld + CNN Appearance Embedding - Hallucinated Neural Radiance Fields in the Wild](https://csj000714.tistory.com/m/1258)
  <!-- END: csj000714 -->
- [`americanopeople` / 복세편살](https://americanopeople.tistory.com/m/)
  - [NotebookLM: AI 시대에 원서 개발 서적 읽기](https://americanopeople.tistory.com/m/462)
  <!-- END: americanopeople -->
- [`lsjsj92` / 꿈 많은 사람의 이야기](https://lsjsj92.tistory.com/m/)
  - [vLLM 사용법 - LLM을 쉽고 빠르게 추론(inference) 및 API 서빙(serving)하기](https://lsjsj92.tistory.com/m/668)
  - [개인화를 고려한 LLM 모델 기반 추천 시스템 - PALR 추천 시스템 논문 리뷰](https://lsjsj92.tistory.com/m/669)
  - [LLM과 추천 시스템을 결합해 설명가능성(Explainability) 제공하기(Feat. LangChain, GPT-4o)](https://lsjsj92.tistory.com/m/670)
  <!-- END: lsjsj92 -->
- [`bahnsville` / nthought](http://bahnsville.tistory.com/m/)
  - [모델 성능이 안 나올 때](http://bahnsville.tistory.com/m/1264)
  - [LLM 왕국에서의 2년 Two Years in LLM](https://bahnsville.tistory.com/m/1265)
  <!-- END: bahnsville -->
- [`mobicon` / AI Convergence](http://mobicon.tistory.com/m/)
  - [\[Embedding\] 중간값과 백터](http://mobicon.tistory.com/m/605)
  - [\[LCC-1\] LangChain Concept - Components & RAG](http://mobicon.tistory.com/m/607)
  - [\[LCC-5\] LangChain VectorStore 이해](https://mobicon.tistory.com/m/614)
  <!-- END: mobicon -->
- [`pearlluck` / 데엔잘하고싶은데엔🔥💎](https://pearlluck.tistory.com/m/)
  - [Triton Inference Server 모델서빙1 - NVIDA Triton(트리톤)이란?](https://pearlluck.tistory.com/m/821)
  - [Embedding을 저장하는 VectorDB 그리고 벡터 유사도 검색 Indexing](https://pearlluck.tistory.com/m/824)
  <!-- END: pearlluck -->
- [`kesakiyo` / 오늘도 개발로그](https://kesakiyo.tistory.com/m/)
  - [미드저니에서 원하는 요소 제거하기: `--no` 옵션과 멀티 프롬프트](https://kesakiyo.tistory.com/m/entry/%EB%AF%B8%EB%93%9C%EC%A0%80%EB%8B%88%EC%97%90%EC%84%9C-%EC%9B%90%ED%95%98%EB%8A%94-%EC%9A%94%EC%86%8C-%EC%A0%9C%EA%B1%B0%ED%95%98%EA%B8%B0-no-%EC%98%B5%EC%85%98%EA%B3%BC-%EB%A9%80%ED%8B%B0-%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8)
  - [미드저니에서 `--chaos`(`--c`) 옵션 사용법: 이미지 다양성 조정 팁](https://kesakiyo.tistory.com/m/entry/%EB%AF%B8%EB%93%9C%EC%A0%80%EB%8B%88%EC%97%90%EC%84%9C-chaos-%EC%98%B5%EC%85%98-%EC%82%AC%EC%9A%A9%EB%B2%95-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EB%8B%A4%EC%96%91%EC%84%B1-%EC%A1%B0%EC%A0%95-%ED%8C%81)
  - [미드저니에서 창의적인 이미지를 생성하는 방법: `--weird`(`--w`) 옵션](https://kesakiyo.tistory.com/m/entry/%EB%AF%B8%EB%93%9C%EC%A0%80%EB%8B%88%EC%97%90%EC%84%9C-%EC%B0%BD%EC%9D%98%EC%A0%81%EC%9D%B8-%EC%9D%B4%EB%AF%B8%EC%A7%80%EB%A5%BC-%EC%83%9D%EC%84%B1%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95-weird-%EC%98%B5%EC%85%98)
  <!-- END: kesakiyo -->
- [`daeson` / 대소니](https://daeson.tistory.com/m/)
  - [Retrieval Augmented Generation (RAG) and Beyond](https://daeson.tistory.com/m/396)
  <!-- END: daeson -->
- [`hl1itj` / 쉽게 살 수 있을까 ?](https://hl1itj.tistory.com/m/)
  - [생성형 AI 시대의 취업 준비](https://hl1itj.tistory.com/m/268)
  <!-- END: hl1itj -->
- [`syaku` / 샤쿠 블로그](https://syaku.tistory.com/m/)
  - [2024년 최신 AI 코딩 도구 완벽 가이드: GitHub Copilot부터 Cursor AI까지 8가지 비교 분석](https://syaku.tistory.com/m/412)
  - [2024 Comprehensive Guide to AI Coding Tools: Comparing 8 Tools from GitHub Copilot to Cursor AI](https://syaku.tistory.com/m/413)
  <!-- END: syaku -->
- [`ravenkim97` / Life Log](https://ravenkim97.tistory.com/m/)
  - [llm 활용 사례](https://ravenkim97.tistory.com/m/511)
  <!-- END: ravenkim97 -->
- [`jeongchul` / Jeongchul Kim](https://jeongchul.tistory.com/m/)
  - [ML Interview - Transfer Learning](https://jeongchul.tistory.com/m/839)
  - [ML Interview - Anomaly Detection](https://jeongchul.tistory.com/m/840)
  - [ML Interview - Z Score](https://jeongchul.tistory.com/m/841)
  - [ML Interview - IQR](https://jeongchul.tistory.com/m/842)
  - [ML Interview - 하이퍼파라미터 튜닝 기법](https://jeongchul.tistory.com/m/843)
  - [ML Interview - Bayesian Optimization](https://jeongchul.tistory.com/m/844)
  - [ML Interview - Evolutionary Algorithms](https://jeongchul.tistory.com/m/845)
  - [ML Interview - Normalization](https://jeongchul.tistory.com/m/846)
  - [ML Interview - 추천 시스템의 고수준 설계](https://jeongchul.tistory.com/m/847)
  - [ML Interview - 모델 추론 실시간 서빙 시스템](https://jeongchul.tistory.com/m/848)
  - [ML Interview - Cross Validation](https://jeongchul.tistory.com/m/849)
  - [ML Interview - 딥러닝의 장점과 단점](https://jeongchul.tistory.com/m/850)
  - [ML Interview - Curse of Dimensionality 차원의 저주](https://jeongchul.tistory.com/m/851)
  - [ML Interview - Parallelism with NVLink](https://jeongchul.tistory.com/m/852)
  <!-- END: jeongchul -->
- [`aspdotnet` / 재우니의 블로그](https://aspdotnet.tistory.com/m/)
  - [2024년 최신 AI 개발 도구 비교: Replit, Bolt.new, Cursor AI](https://aspdotnet.tistory.com/m/3330)
  <!-- END: aspdotnet -->
- [`shplab` / 박서희연구소](https://shplab.tistory.com/m/)
  - [\[Model\] Transformer Model(트랜스포머 모델)](https://shplab.tistory.com/m/entry/Model-Transformer-Model%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EB%AA%A8%EB%8D%B8)
  - [\[Model\] Transformer Model Pipeline(트랜스포머 모델 파이프라인)](https://shplab.tistory.com/m/entry/Model-Transformer-Model-Pipeline%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EB%AA%A8%EB%8D%B8-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8)
  <!-- END: shplab -->
<!-- END: tistory.com -->

---

## Real Python

- [Hugging Face Transformers: Leverage Open-Source AI in Python](https://realpython.com/huggingface-transformers/)
- [Natural Language Processing (NLP) with Python's Natural Language Toolkit (NLTK).](https://realpython.com/nltk-nlp-python/)

<!-- END: realpython.com -->

---

## 정우일 블로그

- [GGUF 파일로 로컬에서 LLM 실행하기](https://wooiljeong.github.io/ml/gguf-llm/)

---

## Daddy Maker

- [LlamaIndex와 LLM 기반 유해, 무해 이미지 인식 기술 개발 방법](https://daddynkidsmakers.blogspot.com/2024/05/llamaindex-llm.html)
- [LLM 기반 그래프 RAG 기술 구현하기](https://daddynkidsmakers.blogspot.com/2024/05/rag.html)
- [대형언어모델 검색증강생성의 핵심기술, 벡터 데이터베이스 Chroma 분석하기](https://daddynkidsmakers.blogspot.com/2024/06/llm-rag-chroma.html)
- [다중 에이전트 LLM 아키텍처 소개](https://daddynkidsmakers.blogspot.com/2024/06/llm.html)
- [LLM 의 통계적 패턴 예측성과 한계에 대한 연구](https://daddynkidsmakers.blogspot.com/2024/06/llm_16.html)
- [NLP의 핵심. 토큰, 임베딩과 파인튜닝](https://daddynkidsmakers.blogspot.com/2024/06/nlp.html)
- [도메인 모델 성능개선을 위한 Lora 기반 LLAMA3 모델 파인튜닝하기](https://daddynkidsmakers.blogspot.com/2024/06/lora-llama3.html)
- [Weights & Biases로 딥러닝 모델 개발 프로세스 기록, 분석, 가시화 및 모델 튜닝하기](https://daddynkidsmakers.blogspot.com/2024/06/weights-biases.html)
- [2024년 오픈소스 대형언어모델 소개](https://daddynkidsmakers.blogspot.com/2024/06/2024.html)
- [AutoRAG 활용 LLM RAG 최적화하기](https://daddynkidsmakers.blogspot.com/2024/07/autorag-llm-rag.html)
- [LLM 기반 센서 데이터 해석 방법](https://daddynkidsmakers.blogspot.com/2024/07/llm_17.html)
- [sLLM과 vLLM에 대한 이야기](https://daddynkidsmakers.blogspot.com/2024/07/sllm-vllm.html)
- [AI 과학자 Sakana.AI 사용법](https://daddynkidsmakers.blogspot.com/2024/08/ai-sakanaai.html)
- [인공지능 딥러닝 모델 성능 지표](https://daddynkidsmakers.blogspot.com/2024/08/blog-post.html)
- [효율적인 딥러닝 모델링을 위한 시계열 데이터 처리](https://daddynkidsmakers.blogspot.com/2024/09/blog-post.html)
- [로컬 멀티모달 LLM 기반 간단한 RAG Enhanced Visual Question Answering 개발하기](https://daddynkidsmakers.blogspot.com/2024/09/llm-rag-enhanced-visual-question.html)

<!-- END: daddynkidsmakers.blogspot.com -->

---

## Outerbounds

- [The Many Ways to Deploy a Model](https://outerbounds.com/blog/the-many-ways-to-deploy-a-model/)

---

## ChristopherGS

- [Running Open Source LLMs In Python - A Practical Guide](https://christophergs.com/blog/running-open-source-llms-in-python)

---

## Simon Willison's TILs

- [Language models on the command-line](https://simonwillison.net/2024/Jun/17/cli-language-models/)
- [Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say](https://simonwillison.net/2024/May/29/training-not-chatting/)
- [NotebookLM’s automatically generated podcasts are surprisingly effective](https://simonwillison.net/2024/Sep/29/notebooklm-audio-overview/)
- [Whisper large-v3-turbo model.](https://simonwillison.net/2024/Oct/1/whisper-large-v3-turbo-model/)
- [Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent](https://simonwillison.net/2024/Oct/17/video-scraping/)

<!-- END: simonwillison.net -->

---

## Allen Pike

- [LLMs Aren’t Just “Trained On the Internet” Anymore](https://allenpike.com/2024/llms-trained-on-internet)

---

## 냉동코더의 기술블로그

- [macOS에서 Ollama 사용하기](https://cliearl.github.io/posts/etc/ollama-intro/)

---

## Applied LLMs

- [What We’ve Learned From A Year of Building with LLMs](https://applied-llms.org/)

---

## OranLooney.com

[A Picture is Worth 170 Tokens: How Does GPT-4o Encode Images?](https://www.oranlooney.com/post/gpt-cnn/)

---

## lytix.ai

- [Cost Of Self Hosting Llama-3 8B-Instruct](https://blog.lytix.co/posts/self-hosting-llama-3)

---

## `llama.ttf`

- [llama.ttf](https://fuglede.github.io/llama.ttf/)

---

## Eugene Yan

- [Patterns for Building LLM-based Systems & Products](https://eugeneyan.com/writing/llm-patterns/)

---

## Alex Strick van Linschoten

- [How to think about creating a dataset for LLM finetuning evaluation](https://mlops.systems/posts/2024-06-25-evaluation-finetuning-manual-dataset.html)

---

## imbue

- [From bare metal to a 70B model: infrastructure set-up and scripts](https://imbue.com/research/70b-infrastructure/)

---

## Jay Alammar

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

---

## The Missing Notes

- [42dot LLM은 리더보드 용도로 파인튜닝한게 아니기 때문에 점수는 낮지만, 실제로 써보면 성능은 매우 뛰어난 걸 확인할 수 있습니다. 320장의 GPU로 약 1주일 동안](https://likejazz.com/post/755660739094724609)
- [현대자동차그룹의 후원으로 MIT 윤킴 교수님과 지난 1년여간 함께 진행해온 산학 연구 논문이 요즘 화제네요.](https://likejazz.com/post/756113715506659328)
- [`llama.cpp`의 K-Quantization https://github.com/ggerganov/llama.cpp/pull/1684 을 따라서 구현해보다가 길을 잃고 😑 좀 더](https://likejazz.com/post/756204304905469952)
- [The Era of 1-bit LLMs: All Large Language Models are in 1.58](https://likejazz.com/post/756294915544219648)
- [간밤에 메타에서 드디어 라마3를 공개했습니다. 역시나 기대했던 대로 압도적인 스케일과 성능을 자랑하네요. 2만 4천 장의 GPU, 15T 학습 데이터, 1천만 건의 인스트럭션](https://likejazz.com/post/756657294178140160)
- [곧 출시되는 ollama의 신규 버전에는 드디어 CJK 문제가 해결되면서 CLI에서 문제 없이 한글 처리가 될 것으로 보이네요. 이외에도 `llama.cpp`에 flash](http://likejazz.com/post/757019683183607808)
- [`llm.c` 프로젝트로 요즘 pure C/CUDA training 코드를 만들고 있는 카파시가 이번에 GPT-2 124M 모델 학습을 1장의 GPU로 90분만에 재현했네요.](http://likejazz.com/post/757200879566946304)

---

## 포티투닷 | 42dot - We Are A Mobility AI Company

- [42dot LLM 1.3B](https://42dot.ai/blog/178)

---

## Haandol, TL;DR

- [SLM 파인튜닝 하기 전에 알아두면 좋은 내용 - 1/2](https://haandol.github.io/2024/07/27/demystifying-small-language-model-fine-tuning.html)
- [AI 에이전트 시스템을 설계할 때 알아두면 좋은 내용](https://haandol.github.io/2024/08/24/architecting-an-agentic-system.html)

---

## 비즈니스, 테크놀로지, 리더십 - CIO Korea

- [엔비디아, 새로운 네모 리트리버 마이크로서비스 발표··· "LLM 정확도 및 처리량 향상 지원"](https://ciokorea.com/news/345545)
- [상용 RAG의 현주소와 도전 과제](https://ciokorea.com/news/345559)
- [“로컬 기기에서 AI 성능 극대화”··· 허깅페이스, 소형 언어 모델 ‘스몰LM’ 공개](https://ciokorea.com/news/344684)
- [카카오, AI 언어모델 성능 평가 데이터셋 구축 및 오픈소스 공개](https://ciokorea.com/news/351362)
- [이참에 알아둘까··· 주요 생성형 AI 용어 23가지](https://ciokorea.com/news/351306)

<!-- END: ciokorea.com -->

---

## 테크놀로지 리더를 위한 글로벌 IT 뉴스 - ITWorld Korea

- [“LLM 시장을 흔드는 메타 라마 3.1” 기업과 솔루션 업체의 득실](https://itworld.co.kr/news/345524)
- [RAG 산업화 여정의 현재와 미래](https://itworld.co.kr/news/345482)
- ["모델에 구애받지 않는 LLM 프롬프트 관리" 프롬프티 시작 가이드](https://itworld.co.kr/news/345734)
- [AI 환상을 현실로 옮기는 가장 효과적인 방법](https://itworld.co.kr/news/346616)
- ["프로젝트 3개 중 1개는 폐기" 생성형 AI 가치 입증이 어려운 이유](https://itworld.co.kr/news/346070)
- [커뮤니티에 공개된 애플 인텔리전스 '비밀 지침'으로 보는 생성형 AI의 과제](https://itworld.co.kr/news/346882)
- [LLM 프로젝트를 프로덕션에 도입하기 위한 5가지 과제와 해결책](https://itworld.co.kr/news/351586)
- [LLM이 아니라 애플리케이션이 필요한 이유](https://itworld.co.kr/news/350963)

<!-- END: itworld.co.kr -->

---

## Speaker Deck | Easily Share Your Presentations Online

- [`huffon` / What if...? 처음부터 다시 LLM 어플리케이션을 개발한다면](https://speakerdeck.com/huffon/what-if-dot-dot-dot-ceoeumbuteo-dasi-llm-eopeulrikeisyeoneul-gaebalhandamyeon)

---

## Dable Tech Blog

- [언어 모델의 Fine-Tuning 성능 올리기](https://teamdable.github.io/techblog/Boosting-Fine-Tuning-of-LM)

---

## Augmend

- [TreeSeg: Hierarchical Topic Segmentation at Augmend](https://augmend.com/blog/TreeSeg)

---

## Min Hsu

- [Scheduling Model in LLVM - Part I](https://myhsu.xyz/llvm-sched-model-1/)

---

## Chip Huyen

- [Building A Generative AI Platform](https://huyenchip.com/2024/07/25/genai-platform.html)

---

## Codesolvent Blog

- [Declarative Programming With AI/LLMs](https://blog.codesolvent.com/2024/09/declarative-programming-with-aillms.html)

---

## 최윤섭의 디지털 헬스케어

- [생성형 의료 인공지능을 ‘새로운 지적 존재’로서 규제하자](https://yoonsupchoi.com/2024/10/03/how-to-regulate-generative-ai-in-healthcare/)

<!-- END: yoonsupchoi.com -->

---

## Timescale Blog

- [RAG Is More Than Just Vector Search](https://timescale.com/blog/rag-is-more-than-just-vector-search/)

<!-- END: timescale.com -->

---

## 000namc.blog

- [AlexNet 논문 리뷰](https://000namc.github.io/2024/10/18/alexnet/)
- [VGG 논문 리뷰](https://000namc.github.io/2024/10/21/vgg/)
- [ResNet 논문 리뷰](https://000namc.github.io/2024/10/23/resnet/)
- [NLP 분야 논문리뷰 및 구현](https://000namc.github.io/2024/10/25/nlp-models/)

<!-- END: 000namc.github.io -->

---

## Sequoia Capital

- [Generative AI’s Act o1 - 에이전트 추론의 시대 개막](https://sequoiacap.com/article/generative-ais-act-o1/)

<!-- END: sequoiacap.com -->

---

## 000namc.xyz

- [NLP 분야 논문리뷰 및 구현](https://blog.000namc.xyz/2024/10/29/nlp-models/)
- [(Transformer) Attention Is All You Need 리뷰](https://blog.000namc.xyz/2024/10/31/transformer/)

<!-- END: blog.000namc.xyz -->

---

## TensorMSA

- [Recent RAG research survey](https://hugrypiggykim.com/2024/11/02/recent-rag-research-survey/)
- [MAMBA](https://hugrypiggykim.com/2024/11/02/mamba/)
- [LLM SUMMARY](https://hugrypiggykim.com/2024/11/02/llm-summary/)

<!-- END: hugrypiggykim.com -->

---

## Jason Kang

- [What Is Direct Parameter Optimization(DPO)?](https://jasonkang14.github.io/llm/what-is-direct-parameter-optimization)

<!-- END: jasonkang14.github.io -->

---

## Pega Devlog

- [생성AI 활용 학회 발표 준비](https://jehyunlee.github.io/2024/11/07/General-67_afore2024/)
- [인공지능을 활용한 슬기로운 연구생활](https://jehyunlee.github.io/2024/11/18/General-69_SNU/)

<!-- END: jehyunlee.github.io -->

---

## DSChloe

- [HuggingFace Login on Google Colab](https://dschloe.github.io/settings/2025/googlecolab_huggingface_login/)
- [Kaggle ML Submission 클래스 만들기](https://dschloe.github.io/settings/2024/12/kaggle_submission_class_sample/)
- [Nasdaq Data Link를 활용한 데이터 수집](https://dschloe.github.io/settings/2024/12/nasdaq_get_data_sample/)

<!-- END: dschloe.github.io -->

<TagLinks/>