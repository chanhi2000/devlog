{
  "channel": {
    "id": "airlab_khu",
    "name": "AIRLab",
    "profile": "https://yt3.googleusercontent.com/LzkY3RU_8_l1puH1JV12IC7ZbQXRg1_RHkRUUITUCmBB8t972N1YBCxEhMUhENoHuKXBR88fBQ=s160-c-k-c0x00ffffff-no-rj",
    "banner": "https://yt3.googleusercontent.com/FODfgc8kQLMgd5WLlNj0FFtdfWUgA3C2LPg88ZvA3mLwP3TQLocxVJPAJwfAZxGL6HZSFGWZuTU=w2276-fcrop64=1,00005a57ffffa5a8-k-c0xffffffff-no-nd-rj"
  },
  "videos": [
    { "id": "KKMxF1zmThY", "title": "Reward?ì–¸ì œê¹Œì§€ ì‚¬ëŒì´ ì¤„ê±°ì•¼? ğŸ’¡Eureka: Human-Level Reward Design via Coding Large Language Models (ICLR 2024)" },
    { "id": "7Ab6QT3Iwdc", "title": "Pointcloud LLMìœ¼ë¡œ ã…†ã„±ã„´ PointLLM Empowering Large Language Models to Understand Point Clouds(ECCV 2024)" },
    { "id": "26Um7tGlH3c", "title": "íŒ¨ì°©ìš”ì¸ì„ ì°¾ìğŸ¤¦â€â™‚ï¸AHA: A VLM for Detecting and Reasoning Over Failures in Robotic Manipulation(ICLR 2025)" },
    { "id": "4EPJvsFA6yc", "title": "Promptë§Œ ì˜ ì¡°ì ˆí•´ë´. ë‹¤ í•  ìˆ˜ ìˆë‹¤ë‹ˆê¹Œ? ğŸ”§ğŸ”§In-Context Learning Enables Robot Action Prediction in LLMs(ICRA 2025)" },
    { "id": "CWVq_t_-Ous", "title": "ì‚¬ëŒëŒ€ì‹  GPT Language Model Guided Concept Bottlenecks for Interpretable Image Classification(CVPR 2023)" },
    { "id": "5UBJD7JmyFE", "title": "ë‚˜ ë¹—ìë£¨ë¡œ ì²­ì†Œí• ë˜ ì›€ì§ì—¬ğŸ§¹MOKA: Open-World Robotic Manipulation through Mark-based Visual Prompting(RSS 2024)" },
    { "id": "POeicCNe8F8", "title": "ì˜¤ì§ ë¡œë´‡ì„ ì›€ì§ì´ê¸° ìœ„í•œ ğŸ¤–ğŸ¤– LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning(CoRL 2024)" },
    { "id": "FmWCwqHuZH4", "title": "ë„ ìŠì–´ ë‹¬ë¼ê³ ? ë„ˆ ëˆ„êµ°ë°? ğŸ’”ğŸ—‘ï¸ Toward Efficient Data-Free Unlearning(AAAI 2025)" },
    { "id": "ZKGqV-ISEh4", "title": "CLS Token ë„ˆ ë­ ë¼? ğŸ¤”ğŸ¤” A Closer Look at the CLS Token for Cross-Domain Few-Shot Learning(NeurIPS 2024)" },
    { "id": "Ecj7EmfasUs", "title": "ë¡œë´‡ê³„ì˜ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì´ ë˜ê²Œì¨! ğŸ¤–ğŸ’ªÏ€0: A Vision-Language-Action Flow Model for General Robot Control(arXiv 2024)" },
    { "id": "O1MKWU76uPc", "title": "VLM... ì´ ë…€ì„ ì²œì¬ì¸ê°€?!ğŸ§ VLMimic: VLMs are Visual Imitation Learner for Fine-grained Actions(NeurIPS 2024)" },
    { "id": "Z1k8zyTVbPk", "title": "VLMì´ ë³¸ëŒ€ë¡œ ë¡œë´‡ì´ í•œë‹¤ê³ ?ğŸ¤–ğŸ¤– Human Demo Video to Robot Action Plan via Vision Language Model(arXiv 2024)" },
    { "id": "ZO9qJo9cIZQ", "title": "ë‚¯ì„¤ì–´?ìµìˆ™í•˜ê²Œ í•´ì¤„ê²Œ Test-Time Style Shifting: Handling Arbitrary Styles in Domain Generalization(ICML 2023)" },
    { "id": "qvHi4jJELOc", "title": "CLIPì˜ ëŠ¥ë ¥, ì–´ë””ê¹Œì§€ ì•Œê³  ìˆë‹ˆ?ğŸ¤“ğŸ¤“ Investigating Compositional Generalization in CLIP Models (ECCV 2024)" },
    { "id": "im7k48jsGpo", "title": "ë­ë“  ê°€ì ¸ì™€! ë‹¤ ë°”ê¿”ì¤„í…Œë‹ˆê¹Œ!!ğŸ“¸ğŸ“¸ StyleShot : A Snapshot on Any Style" },
    { "id": "_xSHs4YTdfs", "title": "ViT? CNN? ì¼ë‹¨ ì„ì–´ë³´ì ViT with Convolutional Multi-scale Feature Interaction for Dense Predictions" },
    { "id": "L1TsLPvpZCU", "title": "ì–¸ì œê¹Œì§€ MLPì— ë©ˆì¶°ìˆì„ ìˆ˜ ì—†ì–ì•„??ğŸ¤”ğŸ¤” KAN: Kolmogorov-Arnold Networks(arXiv 2024)" },
    { "id": "stki3RT2TYE", "title": "ë°”ë¡œ ì •í™•í•œ Depthë¥¼ ì•Œë ¤ì¤˜ë²„ë ¤~ğŸ“ Depth Pro: Sharp Monocular MetricDepth in Less Than a Second" },
    { "id": "pbacEhEJj9I", "title": "3Dë¼ê³  ëª» ë§Œë“¤ ì¤„ ì•Œì•„? ì™€ë„ã„¹ë¼ë¼ğŸ¤“ğŸ¤“MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers" },
    { "id": "al5okaKFpVo", "title": "ì´ê±¸ë¡œ ë­˜ í•˜ê³  ì‹¶ì€ì§€ ì•Œì•„ì•¼ ì˜ ì¡ì§€ GraspGPT: Leveraging Semantic Knowledge from  a LLM for Task-Oriented Grasping" },
    { "id": "wFDYNXC6Hmo", "title": "ê·¸ëƒ¥ ì§€ìš°ê¸°ë§Œ í•˜ëŠ”ê²Œ ì•„ë‹ˆì•¼!!ğŸ—‘ï¸ Unified Gradient-Based MU with Remain Geometry Enhancement(NeurIPS 2024)" },
    { "id": "LsyX8o8iP-E", "title": "ì, ì ˆ ë”°ë¼ì„œ ë¹¼ê³  ë”í•˜ì„¸ìš”. ì°¸ ì‰½ì£ ?ğŸ–Œï¸ğŸ–Œï¸ Domain Gap Embeddings for Generative Dataset Augmentation (CVPR 2024)" },
    { "id": "zPJf8n5ilJU", "title": "ì²˜ìŒ ë³´ëŠ” ê²ƒë„ ì°¾ì•„ì¤„ ìˆ˜ ìˆì§€?ğŸ” Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation (CVPR 2024)" },
    { "id": "u4v0ohC285M", "title": "ë¡œë´‡ì´ ë‹¤ ì¡°ë¦½í•´ì¤€ëŒ€ Multi-level Reasoning for Robotic Assembly: From Sequence Inference to Contact Selection" },
    { "id": "N7KMfOP_AiQ", "title": "ì–´ëŠ ì„¸ì›”ì— ê·¸ê±¸ ë‹¤ í•˜ê³  ìˆë‹ˆ??â±ï¸ Accelerating Diffusion Models via Early Stop of the Diffusion Process" },
    { "id": "J4cKpJnIM1k", "title": "ê·¸ê²ƒì´ ì•Œê³ ì‹¶ë‹¤ VQA Robust Visual Question Answering: Datasets,  Methods, and Future Challenges(TPAMI 2024)" },
    { "id": "-hzcUaIUpzA", "title": "ì´ê²Œ ì˜ ì„ìœ¼ë©´ ë˜ê±°ë©ìš”? í•´ë³´ì„¸ìš”ğŸª„ DIFFUSEMIX: Label-Preserving Data Augmentation with Diffusion Models(CVPR 2024)" },
    
    { "id": "dbEjDN82fw8", "title": "ê¸°ì–µì„ ì‚­ì œí•˜ë¬ì§€, ì„±ëŠ¥ ë§ê³  Towards Efficient Machine Unlearning with Data Augmentation with GLI (CVPRW 2024)" },
    { "id": "yK94PQcsjGM", "title": "ë°ì´í„°ì…‹ ì—†ìœ¼ë©´ ìœ íŠœë¸Œë¼ë„ í„¸ì–´ì„œ ë§Œë“¤ë©´ ë˜. 360Â° Dataset for Depth Prediction and View Synthesis(arXiv 2024)" },
    { "id": "IDdoqW8tGrY", "title": "ìŠ¤í‚¬ì„ ì•Œë ¤ì¤„ê²Œ ë”°ë¼ì™€ Boosting Offline RL for Autonomous Driving with Hierarchical Latent Skills (ICRA 2024)" },
    { "id": "NIOA_WiPKZE", "title": "3ì‹œê°„ ê±¸ë¦´ dataë¥¼ 10ë¶„ë§Œì— ëª¨ì•„ì„œ ê°€ëŠ¥ğŸ‘ MimicPlay: Long-Horizon Imitation Learning by Watching Human Play" },
    { "id": "R4Cxc2zypKU", "title": "ì—†ëŠ” ê³³ ìƒ‰ë„ ì¹ í•´ì£¼ê³  Depthë„ ë§Œë“¤ì–´ì£¼ê³ ğŸ–Œï¸RIC: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction" },
    { "id": "J932O7ef_E0", "title": "ì‘ì•„ë„ ì¶©ë¶„í•´!âš¡EcoTTA: Memory-Efficient Continual Test-time Adaptation (CVPR 2023)" },
    { "id": "EliaxPFMTsA", "title": "í™˜ìë¶„, ê°€ë§Œíˆì¢€ ê³„ì„¸ìš”;; H-ViT: Hierarchical Vision Transformer for Deformable Image Registration (CVPR 2024)" },
    { "id": "kfhj8XHGkK4", "title": "ğŸŒë§¨í•´íŠ¼ì—ì„œ ì†Œì‹¤ì ì„ ì°¾ì•„ë¼!ğŸ” Quasi-Globally Optimal and Real-Time Visual Compass in Manhattan World (RA-L 2022)" },
    { "id": "4ydOpDsztM4", "title": "ì´ë²ˆì—” Depth Task ë°œë™ğŸƒğŸƒ Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data (CVPR 2024)" },
    { "id": "QP9CpI2WV6k", "title": "ë””í“¨ì „ì´ ê·¸ë¦¼ë§Œ ê·¸ë¦¬ê² ë‹ˆ Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation(CVPR 2024)" },
    { "id": "fb3mmr3G_Sk", "title": "ìµœì„ ì…ë‹ˆê¹Œ? í™•ì‹¤í•´ìš”? Can Classic TTA Strategies Be Effectively Applied in Semantic Segmentation?(ACMMM 2024)" },
    { "id": "5c8fPsZzf-w", "title": "ë¹ ë¥´ì§€ë§Œ ê°•ë ¥í•œ ë””í“¨ì „ìœ¼ë¡œ ê°€ìâš¡Generalizable Visuomotor Policy Learning via Simple 3D Representations(RSS 2024)" },
    { "id": "VHQcYKYn13o", "title": "ì´ê²Œ ê¿ˆì¸ì§€ ìƒì‹ ì§€... ğŸŒ™ DataDream: Few-shot Guided Dataset Generation(ECCV 2024)" },
    { "id": "M_OxfwQgy6M", "title": "ì•„ë¬´íŠ¼ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ì§„í™”.ğŸ¤–ğŸ¤–RVT-2 Learning Precise Manipulation from Few Demonstrations(RSS 2024)" },
    { "id": "5jzhUxfdlV0", "title": "ì•„ ì§€í”¼í‹°ì•¼ í•  ìˆ˜ ìˆì–ì•„!! PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs" },
    { "id": "Um9JX2dpyAM", "title": "Targetì´ ì—…ì¨? ì˜ ì°¾ì•„ë´ Geometric MI Approach to Target-Free Camera LiDAR Extrinsic Calibration(WACV 2024)" },
    { "id": "zuOq5f4-7FI", "title": "PAIR360: A Paired Dataset of High-Resolution 360Â° Panoramic Images and LiDAR Scans" }
  ],
  "playlists": [

  ]
}